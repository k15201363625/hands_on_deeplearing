{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hands-on implementation of cnn with numpy-------conv layer\n",
    "> use two method \n",
    "- naive method\n",
    "- fast matrix compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic components\n",
    "- conv_layer\n",
    "- pooling layer\n",
    "- activation layer(relu)\n",
    "- fully connected layer\n",
    "- output layer(softmax for classification problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## faster conv opt\n",
    "> 这里主要使用的就是im2col优化方法：通过将图像展开，使得卷积运算可以变成两个矩阵乘法\n",
    "\n",
    "![conv_img](https://pic1.zhimg.com/80/v2-24557fb0729cacb1e70d30a1fc489150_hd.jpg)\n",
    "> reference:   https://hal.inria.fr/file/index/docid/112631/filename/p1038112283956.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conv_layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conv2d basic params\n",
    "- 输入数据的shape = [N,W,H,C] N=Batchsize/W=width/H=height/C=channels\n",
    "- 卷积核的尺寸ksize ,个数output_channels, kernel shape [output_channels,k,k,C]\n",
    "- 卷积的步长stride，基本默认为1.\n",
    "- 卷积的方法，VALID or SAME，即是否通过padding保持输出图像与输入图像的大小不变(stride==1时候)\n",
    "- $$\\frac{\\partial loss}{\\partial conv\\_out}$$ \n",
    "- $$\\frac{\\partial loss}{\\partial filter},\\frac{\\partial loss}{\\partial bias}$$\n",
    "- 已上分别用self.delta self.filter_grad self.bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyse forward + backward propagation\n",
    "- forward propagation = x\\*w (\\*表示卷积操作) -> output\n",
    "\n",
    "- backward propagation = **padded** output_delta\\* **flipped** w(\\*表示卷积)->input_delta\n",
    "> 实现网络中的一个操作的反向传播，基本操作只需要完成以上这两点\n",
    "> - 1是保证导数信息（残差）继续在网络中向前传播. \n",
    "> - 2是根据传入的导数信息求解自身参数的导数。\n",
    "\n",
    "- backward propagation \n",
    "    - weights\n",
    "        - filter_1 $$\\frac{\\partial (Output\\_features)}{\\partial (Kernel)} = \\frac{\\partial ((Input\\_features)*(Kernel))}{\\partial (Kernel)} = Input\\_features.T$$\n",
    "        - filter_2 $$\\frac{\\partial (loss)}{\\partial (Kernel)} = Input\\_features.T * \\frac{\\partial (loss)}{\\partial (Output\\_features)}$$\n",
    "        - bias $$\\frac{\\partial (loss)}{\\partial (Bias)} = \\sum_{axis=0}(\\frac{\\partial{loss}}{\\partial{Output\\_features}})$$\n",
    "\n",
    "    - delta \n",
    "        - 因为im2col之后的Input Features矩阵和Input本身不是一一对应的（单射) 所以不能用之前的矩阵形式公式进行计算\n",
    "        - 由于只需要统计有贡献的元素 通过卷积过程元素对应情况 结合图像以及公示\n",
    "        - $$\\frac{\\partial loss}{\\partial A} = \\sum _{x=a,...,i} delta_x * fliped\\_kernel[index(delta_x)]$$\n",
    "        - **final conclusion**: **卷积层输入的delta计算 等于 卷积层输出的delta与翻转后的Kernel进行卷积(deconv)**\n",
    "        - NOTE： 在dconv过程中需要注意padding(stride,padding，边角问题)\n",
    "            - 如果是stride!=1 需要在内部进行padding (行列间padding)\n",
    "            - 如果stride==1 只需要在边角padding\n",
    "            - 其中边角padding有\n",
    "                - same : **ksize/2**\n",
    "                - valid : **ksize-1**\n",
    "            - 无论怎样对于delta进行padding 但是在dconv过程需要使用**stride=1**\n",
    "        - reference：\n",
    "            - https://zhuanlan.zhihu.com/p/33802329\n",
    "            - https://www.jianshu.com/p/b6b9a1b0aabf\n",
    "            - 上面两个讲反向传播很好 一个代码 一个公式 \n",
    "    \n",
    "    - NOTE:代码实现多一维batchsize，一个batch里面的w_gradient和b_gradient则是对该batch求和得到的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(object):\n",
    "    def __init__(self,input_shape,output_channels,ksize,stride=1,padding='same'):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_channels = input_shape[-1]\n",
    "        self.output_channels = output_channels\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        # default batch_size = shape[0]\n",
    "        self.batch_size = input_shape[0]\n",
    "        \n",
    "        # initialize params with standardnormal + scaler\n",
    "        # NOTE:msra方法设置scaler\n",
    "        weight_scaler = math.sqrt(ksize**2*self.input_channels/2)\n",
    "        self.filter = np.random.randn(self.output_channels,ksize,ksize,self.input_channels)/weight_scaler\n",
    "        self.bias = np.random.randn(self.output_channels)/weight_scaler\n",
    "        \n",
    "        # back propagation params\n",
    "        shape = self.input_shape\n",
    "        if padding == 'valid':\n",
    "            self.delta = np.zeros((shape[0],(shape[1]-ksize+1)//self.stride,\n",
    "                                  (shape[2]-ksize+1)//self.stride,self.output_channels))\n",
    "        elif padding == 'same':\n",
    "            self.delta = np.zeros((shape[0],shape[1]//self.stride,\n",
    "                                   shape[2]//self.stride,self.output_channels))   \n",
    "        else:\n",
    "            raise AttributeError('unsupported padding method str')\n",
    "        self.output_shape = self.delta.shape\n",
    "        print(self.output_shape)\n",
    "        self.filter_grad = np.zeros(self.filter.shape)\n",
    "        self.bias_grad = np.zeros(self.bias.shape)\n",
    "    def _conv(self,x):\n",
    "        \"\"\"\n",
    "        faster convolution with matrix method than naive dot product(element-wise)\n",
    "        four steps:\n",
    "        1.reshape filter\n",
    "        2.padding\n",
    "        3.im2col save image\n",
    "        4.matmul(input_features,kernel_matrix)=output_features\n",
    "        \"\"\"\n",
    "        # shape=(self.output_channels,ksize,ksize,self.input_channels)\n",
    "        col_filter = np.transpose(self.filter,[1,2,3,0])\n",
    "        col_filter = col_filter.reshape([-1,self.output_channels])\n",
    "        if self.padding == 'same':\n",
    "            x = np.pad(x,((0,0),(self.ksize//2,self.ksize//2),(self.ksize//2,self.ksize//2),(0,0)),\n",
    "                      mode='constant',constant_values = 0)\n",
    "        # 整个batch一起处理\n",
    "        #self.img_cols = self._img2col(x)\n",
    "\n",
    "        # 每个sample in batch 分别处理\n",
    "        self.img_cols = []\n",
    "        self.conv_out = []\n",
    "        for i in range(self.batch_size):\n",
    "            img_i = x[i][np.newaxis,:] # 保障4dim\n",
    "            nowcol = self._img2col(img_i,self.ksize,self.stride)\n",
    "            self.img_cols.append(nowcol)\n",
    "            self.conv_out.append(np.reshape(\n",
    "                np.dot(nowcol,col_filter)+self.bias,\n",
    "                self.delta[0].shape))\n",
    "            \n",
    "        self.img_cols = np.array(self.img_cols)\n",
    "        self.conv_out = np.array(self.conv_out)\n",
    "        return self.conv_out\n",
    "    \n",
    "    def _img2col(self,image,ksize,stride):\n",
    "        \"\"\"tool function of transform img-> matrix\n",
    "        only to one sample\n",
    "        otherwise:reshape 等操作需要改变处理方式\n",
    "        \"\"\"\n",
    "        # image is a 4d tensor([batchsize, width ,height, channel])\n",
    "        img_cols = []\n",
    "        for i in range(0,image.shape[1]-ksize+1,stride):\n",
    "            for j in range(0,image.shape[2]-ksize+1,stride):\n",
    "                nowcol = image[:,i:i+ksize,j:j+ksize,:].reshape([-1])\n",
    "                img_cols.append(nowcol)\n",
    "        img_cols = np.array(img_cols)\n",
    "        return img_cols\n",
    "    \n",
    "    def forward_propagate(self,x):\n",
    "        return self._conv(x)\n",
    "    \n",
    "    def _dconv(self):\n",
    "        \"\"\"\n",
    "        deconv of padded eta with flippd kernel to get next_eta\n",
    "        \"\"\"\n",
    "        if self.padding == 'valid':\n",
    "            pad_delta = np.pad(self.delta,\n",
    "                              ((0,0),(self.ksize-1,self.ksize-1),(self.ksize-1,self.ksize-1),(0,0)),\n",
    "                              mode='constant',constant_values=0)\n",
    "            \n",
    "        elif self.padding == 'same':\n",
    "            pad_delta = np.pad(self.delta,\n",
    "                              ((0,0),(self.ksize//2,self.ksize//2),(self.ksize//2,self.ksize//2),(0,0)),\n",
    "                              mode='constant',constant_values=0)\n",
    "        # only to 0,1 dims (fliplr,flipud)\n",
    "        # 使用swapaxes与transpose类似功能 but只能交换两个维度\n",
    "        # (kszie,ksize,output_channels,input_channels)\n",
    "        flipped_filter = np.transpose(self.filter,[1,2,0,3])\n",
    "        flipped_filter = np.fliplr(np.flipud(flipped_filter))\n",
    "        col_flipped_filter = flipped_filter.reshape([-1,self.input_channels])\n",
    "        # delta img2col with ** list generator **\n",
    "        col_pad_delta = np.array(\n",
    "            [self._img2col(pad_delta[i][np.newaxis,:],\n",
    "                           self.ksize,self.stride) for i in range(self.batch_size)])\n",
    "        # dconv (matmul)\n",
    "        input_delta = np.dot(col_pad_delta,col_flipped_filter)\n",
    "        # 直接reshape就可以实现 因为已经分开batch处理了\n",
    "        input_delta = input_delta.reshape(self.input_shape)\n",
    "        return input_delta\n",
    "    \n",
    "    def update_params(self,learning_rate=1e-5, weight_decay=1e-4):\n",
    "        \"\"\"\n",
    "        use L2 regularization \n",
    "        并且使用了新的更新方式 ： simple sgd + weight decay\n",
    "        \"\"\"\n",
    "        # weight_decay = L2 regularization\n",
    "        self.filter *= (1 - weight_decay)\n",
    "        self.bias *= (1 - weight_decay)\n",
    "        self.filter -= learning_rate * self.filter_grad\n",
    "        self.bias -= learning_rate * self.bias_grad\n",
    "        \n",
    "    def backward_propagate(self,delta,learning_rate=1e-5, weight_decay=1e-4):\n",
    "        \"\"\"\n",
    "        deconv of padded delta with flippd kernel to get next_delta\n",
    "        four steps:\n",
    "        1.对输入的delta根据method进行padding操作。\n",
    "        2.对卷积核参数进行翻转。\n",
    "        3.对卷积核进行转置操作。(小心 numpy运行的未必是想象的结果)\n",
    "        4.对翻转后的卷积核与padding后的pad_delta进行卷积计算，方法同conv.forward\n",
    "        \"\"\"\n",
    "        self.delta = delta\n",
    "        # reshape\n",
    "        col_delta = np.reshape(self.delta,\n",
    "                              [self.batch_size,-1,self.output_channels])\n",
    "        \n",
    "        # 还原 保障\n",
    "        self.filter_grad = np.zeros(self.filter.shape)\n",
    "        self.bias_grad = np.zeros(self.bias.shape)\n",
    "        \n",
    "        # weights gradient update\n",
    "        for i in range(self.batch_size):\n",
    "            self.filter_grad += np.dot(self.img_cols[i].T,\n",
    "                                       col_delta[i]).reshape(self.filter.shape) \n",
    "        self.bias_grad+=np.sum(col_delta,axis=(0,1))\n",
    "        \n",
    "        self.update_params(learning_rate,weight_decay)\n",
    "\n",
    "        \n",
    "        # dconv to backpropagation grad\n",
    "        return self._dconv()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.random.standard_normal((1,2)))\n",
    "# print(np.random.randn(1,2))\n",
    "# demo\n",
    "old = np.array([[[1],[3]],[[1],[3]],[[1],[3]]])\n",
    "print(old.shape)\n",
    "new = np.transpose(old,axes=[0,2,1])\n",
    "print(new.shape)\n",
    "print(new)\n",
    "new = np.transpose(old,axes=[2,1,0])\n",
    "print(new.shape)\n",
    "new = np.transpose(old,axes=[1,0,2])\n",
    "print(new.shape)\n",
    "print(np.reshape([[1,2],[3,4]],[-1]).shape)\n",
    "np.array([[1,2],[3,4]])\n",
    "a = np.array([1,2,3])\n",
    "b = a.reshape([1,-1])\n",
    "print(a.shape,b.shape)\n",
    "print(a is b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 32, 12)\n",
      "(12, 3, 3, 3)\n",
      "(12,)\n",
      "(2, 32, 32, 12)\n"
     ]
    }
   ],
   "source": [
    "# test conv layer\n",
    "if __name__ == '__main__':\n",
    "    img = np.ones((2,32,32,3))\n",
    "    conv = Conv2D(img.shape,12,3,1,'same')\n",
    "    conv_out = conv.forward_propagate(img)\n",
    "    # 模拟loss_delta\n",
    "    out_delta = conv_out.copy() * 2\n",
    "    conv.backward_propagate(out_delta)\n",
    "    print(conv.filter_grad.shape)\n",
    "    print(conv.bias_grad.shape)\n",
    "    print(conv_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
