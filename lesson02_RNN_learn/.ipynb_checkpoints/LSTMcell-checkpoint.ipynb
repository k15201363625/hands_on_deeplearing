{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lesson2 - hands on implementation RNN ------ LSTMcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm(long short memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM basic formulas\n",
    "- $$\\mathbf{f}_t=\\sigma(W_f\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_f)\\qquad\\quad(式1)$$\n",
    "- $$\\begin{align}\n",
    "\\begin{bmatrix}W_f\\end{bmatrix}\\begin{bmatrix}\\mathbf{h}_{t-1}\\\\\n",
    "\\mathbf{x}_t\\end{bmatrix}&=\n",
    "\\begin{bmatrix}W_{fh}&W_{fx}\\end{bmatrix}\\begin{bmatrix}\\mathbf{h}_{t-1}\\\\\n",
    "\\mathbf{x}_t\\end{bmatrix}\\\\\n",
    "&=W_{fh}\\mathbf{h}_{t-1}+W_{fx}\\mathbf{x}_t\n",
    "\\end{align}$$\n",
    "- $$\\mathbf{i}_t=\\sigma(W_i\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_i)\\qquad\\quad(式2)$$\n",
    "- $$\\mathbf{\\tilde{c}}_t=\\tanh(W_c\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_c)\\qquad\\quad(式3)$$\n",
    "- $$\\mathbf{c}_t=f_t\\circ{\\mathbf{c}_{t-1}}+i_t\\circ{\\mathbf{\\tilde{c}}_t}\\qquad\\quad(式4)$$\n",
    "- $$\\mathbf{o}_t=\\sigma(W_o\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_o)\\qquad\\quad(式5)$$\n",
    "- $$\\mathbf{h}_t=\\mathbf{o}_t\\circ \\tanh(\\mathbf{c}_t)\\qquad\\quad(式6)$$\n",
    "### math basic knowledge expansion\n",
    "- 元素乘法作用与两个向量 $$\\mathbf{a}\\circ\\mathbf{b}=\\begin{bmatrix}\n",
    "a_1\\\\a_2\\\\a_3\\\\...\\\\a_n\n",
    "\\end{bmatrix}\\circ\\begin{bmatrix}\n",
    "b_1\\\\b_2\\\\b_3\\\\...\\\\b_n\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "a_1b_1\\\\a_2b_2\\\\a_3b_3\\\\...\\\\a_nb_n\n",
    "\\end{bmatrix}$$\n",
    "- 元素乘法作用于向量与矩阵\n",
    "$$\\begin{align}\n",
    "\\mathbf{a}\\circ X&=\\begin{bmatrix}\n",
    "a_1\\\\a_2\\\\a_3\\\\...\\\\a_n\n",
    "\\end{bmatrix}\\circ\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} & ... & x_{1n}\\\\\n",
    "x_{21} & x_{22} & x_{23} & ... & x_{2n}\\\\\n",
    "x_{31} & x_{32} & x_{33} & ... & x_{3n}\\\\\n",
    "& & ...\\\\\n",
    "x_{n1} & x_{n2} & x_{n3} & ... & x_{nn}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "a_1x_{11} & a_1x_{12} & a_1x_{13} & ... & a_1x_{1n}\\\\\n",
    "a_2x_{21} & a_2x_{22} & a_2x_{23} & ... & a_2x_{2n}\\\\\n",
    "a_3x_{31} & a_3x_{32} & a_3x_{33} & ... & a_3x_{3n}\\\\\n",
    "& & ...\\\\\n",
    "a_nx_{n1} & a_nx_{n2} & a_nx_{n3} & ... & a_nx_{nn}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "- 作用于两个矩阵 两个矩阵对应位置元素相乘\n",
    "- 所以得到了在RNN中应用广泛的两个公式 (**有关元素乘法** 与 **对角矩阵**)\n",
    "    - 一个对角矩阵右乘一个矩阵时，相当于用对角矩阵的对角线组成的向量按元素乘那个矩阵$$diag[\\mathbf{a}]X=\\mathbf{a}\\circ X$$\n",
    "    - 当一个行向量右乘一个对角矩阵时，相当于这个行向量按元素乘那个矩阵对角线组成的向量$$\\mathbf{a}^Tdiag[\\mathbf{b}]=\\mathbf{a}\\circ\\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播注意点\n",
    "- 反向传播的delta定义不再是针对加权输入net的 而是针对与lstm的输出h，因为如果针对net则需要有4个加权的net，而我们只想向上层传播一个误差而不是四个误差,所以有了现在的delta的定义$$\\delta_t\\overset{def}{=}\\frac{\\partial{E}}{\\partial{\\mathbf{h}_t}}$$\n",
    "- 当然其他的有关加权输入的偏导数都可以通过这个delta计算得到 这里进行定义$$\\begin{align}\n",
    "\\mathbf{net}_{f,t}&=W_f[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_f\\\\\n",
    "&=W_{fh}\\mathbf{h}_{t-1}+W_{fx}\\mathbf{x}_t+\\mathbf{b}_f\\\\\n",
    "\\mathbf{net}_{i,t}&=W_i[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_i\\\\\n",
    "&=W_{ih}\\mathbf{h}_{t-1}+W_{ix}\\mathbf{x}_t+\\mathbf{b}_i\\\\\n",
    "\\mathbf{net}_{\\tilde{c},t}&=W_c[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_c\\\\\n",
    "&=W_{ch}\\mathbf{h}_{t-1}+W_{cx}\\mathbf{x}_t+\\mathbf{b}_c\\\\\n",
    "\\mathbf{net}_{o,t}&=W_o[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_o\\\\\n",
    "&=W_{oh}\\mathbf{h}_{t-1}+W_{ox}\\mathbf{x}_t+\\mathbf{b}_o\\\\\n",
    "\\delta_{f,t}&\\overset{def}{=}\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{f,t}}}\\\\\n",
    "\\delta_{i,t}&\\overset{def}{=}\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{i,t}}}\\\\\n",
    "\\delta_{\\tilde{c},t}&\\overset{def}{=}\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{\\tilde{c},t}}}\\\\\n",
    "\\delta_{o,t}&\\overset{def}{=}\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{o,t}}}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播详解\n",
    "- 反向传播分为两个部分\n",
    "    - 误差项沿时间 bp\n",
    "    - 误差项沿层次结构上传(多层LSTM)\n",
    "- 反向传播同时需要计算对于权重的梯度\n",
    "    - 对于w_h 对于b 对于w_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 沿着时间反向传播 \n",
    "- 经过推导得到：\n",
    "$$\\begin{align}\n",
    "\\delta_{t-1}&=\\delta_{o,t}^T\\frac{\\partial{\\mathbf{net}_{o,t}}}{\\partial{\\mathbf{h_{t-1}}}}\n",
    "+\\delta_{f,t}^T\\frac{\\partial{\\mathbf{net}_{f,t}}}{\\partial{\\mathbf{h_{t-1}}}}\n",
    "+\\delta_{i,t}^T\\frac{\\partial{\\mathbf{net}_{i,t}}}{\\partial{\\mathbf{h_{t-1}}}}\n",
    "+\\delta_{\\tilde{c},t}^T\\frac{\\partial{\\mathbf{net}_{\\tilde{c},t}}}{\\partial{\\mathbf{h_{t-1}}}}\\\\\n",
    "&=\\delta_{o,t}^T W_{oh}\n",
    "+\\delta_{f,t}^TW_{fh}\n",
    "+\\delta_{i,t}^TW_{ih}\n",
    "+\\delta_{\\tilde{c},t}^TW_{ch}\\qquad\\quad(式8)\\\\\n",
    "\\end{align}$$\n",
    "- $$\\begin{align}\n",
    "\\delta_{o,t}^T&=\\delta_t^T\\circ\\tanh(\\mathbf{c}_t)\\circ\\mathbf{o}_t\\circ(1-\\mathbf{o}_t)\\qquad\\quad(式9)\\\\\n",
    "\\delta_{f,t}^T&=\\delta_t^T\\circ\\mathbf{o}_t\\circ(1-\\tanh(\\mathbf{c}_t)^2)\\circ\\mathbf{c}_{t-1}\\circ\\mathbf{f}_t\\circ(1-\\mathbf{f}_t)\\qquad(式10)\\\\\n",
    "\\delta_{i,t}^T&=\\delta_t^T\\circ\\mathbf{o}_t\\circ(1-\\tanh(\\mathbf{c}_t)^2)\\circ\\mathbf{\\tilde{c}}_t\\circ\\mathbf{i}_t\\circ(1-\\mathbf{i}_t)\\qquad\\quad(式11)\\\\\n",
    "\\delta_{\\tilde{c},t}^T&=\\delta_t^T\\circ\\mathbf{o}_t\\circ(1-\\tanh(\\mathbf{c}_t)^2)\\circ\\mathbf{i}_t\\circ(1-\\mathbf{\\tilde{c}}^2)\\qquad\\quad(式12)\\\\\n",
    "\\end{align}$$\n",
    "- 从而得到传递到任何时刻的公式\n",
    "$$\\delta_k^T=\\prod_{j=k}^{t-1}\\delta_{o,j}^TW_{oh}\n",
    "+\\delta_{f,j}^TW_{fh}\n",
    "+\\delta_{i,j}^TW_{ih}\n",
    "+\\delta_{\\tilde{c},j}^TW_{ch}\\qquad\\quad(式13)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 误差传递到上层(此时delta的定义可以针对加权输入了)\n",
    "- 假设当前为第l层，定义l-1层的误差项是误差函数对l-1层加权输入的导数$$\\delta_t^{l-1}\\overset{def}{=}\\frac{\\partial{E}}{\\mathbf{net}_t^{l-1}}$$\n",
    "$$\\mathbf{x}_t^l=f^{l-1}(\\mathbf{net}_t^{l-1})$$\n",
    "- 由于l层4个加权输入都是$x_t$的函数，而$x_t$是$net_t^{l-1}$的函数 所以有$$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{net}_t^{l-1}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{\\mathbf{net}_{f,t}^l}}}\\frac{\\partial{\\mathbf{\\mathbf{net}_{f,t}^l}}}{\\partial{\\mathbf{x}_t^l}}\\frac{\\partial{\\mathbf{x}_t^l}}{\\partial{\\mathbf{\\mathbf{net}_t^{l-1}}}}\n",
    "+\\frac{\\partial{E}}{\\partial{\\mathbf{\\mathbf{net}_{i,t}^l}}}\\frac{\\partial{\\mathbf{\\mathbf{net}_{i,t}^l}}}{\\partial{\\mathbf{x}_t^l}}\\frac{\\partial{\\mathbf{x}_t^l}}{\\partial{\\mathbf{\\mathbf{net}_t^{l-1}}}}\n",
    "+\\frac{\\partial{E}}{\\partial{\\mathbf{\\mathbf{net}_{\\tilde{c},t}^l}}}\\frac{\\partial{\\mathbf{\\mathbf{net}_{\\tilde{c},t}^l}}}{\\partial{\\mathbf{x}_t^l}}\\frac{\\partial{\\mathbf{x}_t^l}}{\\partial{\\mathbf{\\mathbf{net}_t^{l-1}}}}\n",
    "+\\frac{\\partial{E}}{\\partial{\\mathbf{\\mathbf{net}_{o,t}^l}}}\\frac{\\partial{\\mathbf{\\mathbf{net}_{o,t}^l}}}{\\partial{\\mathbf{x}_t^l}}\\frac{\\partial{\\mathbf{x}_t^l}}{\\partial{\\mathbf{\\mathbf{net}_t^{l-1}}}}\\\\\n",
    "&=\\delta_{f,t}^TW_{fx}\\circ f'(\\mathbf{net}_t^{l-1})+\\delta_{i,t}^TW_{ix}\\circ f'(\\mathbf{net}_t^{l-1})+\\delta_{\\tilde{c},t}^TW_{cx}\\circ f'(\\mathbf{net}_t^{l-1})+\\delta_{o,t}^TW_{ox}\\circ f'(\\mathbf{net}_t^{l-1})\\\\\n",
    "&=(\\delta_{f,t}^TW_{fx}+\\delta_{i,t}^TW_{ix}+\\delta_{\\tilde{c},t}^TW_{cx}+\\delta_{o,t}^TW_{ox})\\circ f'(\\mathbf{net}_t^{l-1})\\qquad\\quad(式14)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 权重梯度计算\n",
    "- 通过basicrnn中的推导 得到权重的梯度是各个时间的求和 所以先求得单个时间的梯度 在进行求和 得到最终的梯度\n",
    "- 对于state权重矩阵w_h:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{W_{oh,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{o,t}}}\\frac{\\partial{\\mathbf{net}_{o,t}}}{\\partial{W_{oh,t}}}\\\\\n",
    "&=\\delta_{o,t}\\mathbf{h}_{t-1}^T\\\\\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W_{fh,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{f,t}}}\\frac{\\partial{\\mathbf{net}_{f,t}}}{\\partial{W_{fh,t}}}\\\\\n",
    "&=\\delta_{f,t}\\mathbf{h}_{t-1}^T\\\\\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W_{ih,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{i,t}}}\\frac{\\partial{\\mathbf{net}_{i,t}}}{\\partial{W_{ih,t}}}\\\\\n",
    "&=\\delta_{i,t}\\mathbf{h}_{t-1}^T\\\\\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W_{ch,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{\\tilde{c},t}}}\\frac{\\partial{\\mathbf{net}_{\\tilde{c},t}}}{\\partial{W_{ch,t}}}\\\\\n",
    "&=\\delta_{\\tilde{c},t}\\mathbf{h}_{t-1}^T\\\\\n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{W_{oh}}}&=\\sum_{j=1}^t\\delta_{o,j}\\mathbf{h}_{j-1}^T\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W_{fh}}}&=\\sum_{j=1}^t\\delta_{f,j}\\mathbf{h}_{j-1}^T\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W_{ih}}}&=\\sum_{j=1}^t\\delta_{i,j}\\mathbf{h}_{j-1}^T\\\\\n",
    "\\frac{\\partial{E}}{\\partial{W_{ch}}}&=\\sum_{j=1}^t\\delta_{\\tilde{c},j}\\mathbf{h}_{j-1}^T\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "- 对于输入权重矩阵w_x:与上文类似\n",
    "- 对与偏置bias:$$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_{o,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{o,t}}}\\frac{\\partial{\\mathbf{net}_{o,t}}}{\\partial{\\mathbf{b}_{o,t}}}\\\\\n",
    "&=\\delta_{o,t}\\\\\\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_{f,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{f,t}}}\\frac{\\partial{\\mathbf{net}_{f,t}}}{\\partial{\\mathbf{b}_{f,t}}}\\\\\n",
    "&=\\delta_{f,t}\\\\\\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_{i,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{i,t}}}\\frac{\\partial{\\mathbf{net}_{i,t}}}{\\partial{\\mathbf{b}_{i,t}}}\\\\\n",
    "&=\\delta_{i,t}\\\\\\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_{c,t}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{\\tilde{c},t}}}\\frac{\\partial{\\mathbf{net}_{\\tilde{c},t}}}{\\partial{\\mathbf{b}_{c,t}}}\\\\\n",
    "&=\\delta_{\\tilde{c},t}\\\\\n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_o}}&=\\sum_{j=1}^t\\delta_{o,j}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_i}}&=\\sum_{j=1}^t\\delta_{i,j}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_f}}&=\\sum_{j=1}^t\\delta_{f,j}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}_c}}&=\\sum_{j=1}^t\\delta_{\\tilde{c},j}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model importing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def element_wise_op(array,op):\n",
    "    \"\"\"\n",
    "    对于numpy类型data进行element wise 的op函数对应的操作\n",
    "    \"\"\"\n",
    "    for i in np.nditer(array,op_flags=['readwrite']):\n",
    "        i[...] = op(i)\n",
    "        \n",
    "class SigmoidActivator(object):\n",
    "    def forward(self,x):\n",
    "        return 1.0 / (1.0+np.exp(-x))\n",
    "    def backward(self,out):\n",
    "        return out * (1-out)\n",
    "    \n",
    "class TanhActivator(object):\n",
    "    def forward(self,x):\n",
    "        return 2.0 / (1.0+np.exp(-2*x)) - 1.0\n",
    "    def backward(self,out):\n",
    "        # return 1 - np.square(out)\n",
    "        return 1 - out*out\n",
    "        \n",
    "class IdentityActivator(object):\n",
    "    \"\"\"\n",
    "    多用于gradient check 中的activation function\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    def backward(self,out):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer(object):\n",
    "    def __init__(self,input_size,state_size,\n",
    "                learning_rate,weight_decay):\n",
    "        self.input_size = input_size\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.gate_activator = SigmoidActivator()\n",
    "        self.output_activator = TanhActivator()\n",
    "        self.input_activator = TanhActivator()\n",
    "        \n",
    "        # 当前传播到的时间\n",
    "        self.times = 0\n",
    "        \n",
    "        # 初始化各个状态 以及输入等的cache列表\n",
    "        self.input_list = [np.zeros((self.input_size,1))]\n",
    "        # 各个时刻的单元状态向量c\n",
    "        self.c_list = self._init_state_vec_list()\n",
    "        # 各个时刻的输出向量h\n",
    "        self.h_list = self._init_state_vec_list()\n",
    "        # 各个时刻的遗忘门f\n",
    "        self.f_list = self._init_state_vec_list()\n",
    "        # 各个时刻的输入门i\n",
    "        self.i_list = self._init_state_vec_list()\n",
    "        # 各个时刻的输出门o\n",
    "        self.o_list = self._init_state_vec_list()\n",
    "        # 各个时刻的即时状态c~\n",
    "        self.ct_list = self._init_state_vec_list()\n",
    "        \n",
    "        # 初始化权重矩阵 对于权重矩阵采用分开处理的思想 类似于basicrnn\n",
    "        # 遗忘门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wfh, self.Wfx, self.bf = (\n",
    "            self._init_weights())\n",
    "        # 输入门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wih, self.Wix, self.bi = (\n",
    "            self._init_weights())\n",
    "        # 输出门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Woh, self.Wox, self.bo = (\n",
    "            self._init_weights())\n",
    "        # 单元状态权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wch, self.Wcx, self.bc = (\n",
    "            self._init_weights())\n",
    "        \n",
    "    def _init_state_vec_list(self):\n",
    "        \"\"\"initialize state vector list\n",
    "        采用和basicrnn一样的处理方法 在开始填充空站位元素\n",
    "        \"\"\"\n",
    "        state_vec_list = []\n",
    "        state_vec_list.append(np.zeros((self.state_size,1)))\n",
    "        return state_vec_list\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        initialize weight matrix and bias (weights)\n",
    "        return wh wx bias\n",
    "        default use uniform distribution to w and 0 to bias\n",
    "        \"\"\"\n",
    "        wh = np.random.uniform(-1e-2,1e-2,(self.state_size,self.state_size))\n",
    "        wx = np.random.uniform(-1e-2,1e-2,(self.state_size,self.input_size))\n",
    "        bias = np.zeros((self.state_size,1))\n",
    "        return wh,wx,bias\n",
    "    \n",
    "    def _init_delta_list(self):\n",
    "        \"\"\"initialize delta list\n",
    "        different from init_state_vec_list\n",
    "        append 0-self.times 个delta vec\n",
    "        \"\"\"\n",
    "        delta_list = [np.zeros((self.state_size,1)) for i in range(self.times+1)]\n",
    "        return delta_list\n",
    "    \n",
    "    def _init_weights_grad(self):\n",
    "        \"\"\"\n",
    "        initialize weight matrix and bias (weights)\n",
    "        return wh wx bias\n",
    "        default use uniform distribution to w and 0 to bias\n",
    "        \"\"\"\n",
    "        wh = np.zeros((self.state_size,self.state_size))\n",
    "        wx = np.zeros((self.state_size,self.input_size))\n",
    "        bias = np.zeros((self.state_size,1))\n",
    "        return wh,wx,bias\n",
    "    \n",
    "    # forward as six formula\n",
    "    def forward(self,x):\n",
    "        self.input_list.append(x)\n",
    "        self.times += 1\n",
    "         # forget gate\n",
    "        fg = self._calc_local_out(x, self.Wfh, self.Wfx, \n",
    "            self.bf, self.gate_activator)\n",
    "        self.f_list.append(fg)\n",
    "        # input gate\n",
    "        ig = self._calc_local_out(x, self.Wih, self.Wix,\n",
    "            self.bi, self.gate_activator)\n",
    "        self.i_list.append(ig)\n",
    "        # output gate\n",
    "        og = self._calc_local_out(x, self.Woh, self.Wox,\n",
    "            self.bo, self.gate_activator)\n",
    "        self.o_list.append(og)\n",
    "        # now generated state \n",
    "        ct = self._calc_local_out(x, self.Wch, self.Wcx,\n",
    "            self.bc, self.input_activator)\n",
    "        self.ct_list.append(ct)\n",
    "        # real now state [element-wise-product]\n",
    "        c = fg*self.c_list[self.times-1] + ig*ct\n",
    "        self.c_list.append(c)\n",
    "        # output\n",
    "        h = og*self.output_activator.forward(c)\n",
    "        self.h_list.append(h)\n",
    "    \n",
    "    def _calc_local_out(self,x,wh,wx,b,activator):\n",
    "        \"\"\"calculate out of gate or now generated state\n",
    "        net value\n",
    "        \"\"\"\n",
    "        lasth = self.h_list[self.times-1]\n",
    "        # net stand for weighted inputs\n",
    "#         print(lasth.shape,x.shape)\n",
    "        net = np.dot(wh,lasth) + np.dot(wx,x) + b\n",
    "        local_out = activator.forward(net)\n",
    "        return local_out\n",
    "    \n",
    "    def backward(self,delta_h,activator):\n",
    "        \"\"\"反向传播 实现训练\n",
    "        特别注意 这里的delta是针对h的 而不是针对各个net的\n",
    "        \"\"\"\n",
    "        self.bp_delta(delta_h,activator)\n",
    "        self.calc_gradient()\n",
    "    \n",
    "    def bp_delta(self,delta_h,activator):\n",
    "        \"\"\"calc bp_deltas of all time step\n",
    "        1. init\n",
    "        2. get last delta and save\n",
    "        3. bp_delta\n",
    "        \"\"\"\n",
    "        self.delta_h_list = self._init_delta_list()\n",
    "        self.delta_o_list = self._init_delta_list()\n",
    "        self.delta_i_list = self._init_delta_list()  \n",
    "        self.delta_f_list = self._init_delta_list()\n",
    "        self.delta_ct_list = self._init_delta_list() \n",
    "    \n",
    "        self.delta_h_list[-1] = delta_h\n",
    "        \n",
    "        for k in range(self.times,0,-1):\n",
    "            self.bp_delta_k(k)\n",
    "            \n",
    "    def bp_delta_k(self,k):\n",
    "        \"\"\"one step(k->k-1) bp_delta \n",
    "        get k-1 step ---- delta_h and delta_net\n",
    "        \"\"\"\n",
    "        # get delta_h_k\n",
    "        delta_h_k = self.delta_h_list[k]\n",
    "        \n",
    "        # k step前向计算的值\n",
    "        ig = self.i_list[k]\n",
    "        og = self.o_list[k]\n",
    "        fg = self.f_list[k]\n",
    "        ct = self.ct_list[k]\n",
    "        c = self.c_list[k]\n",
    "        c_prev = self.c_list[k-1]\n",
    "        tanh_c = self.output_activator.forward(c) #可避免多次计算\n",
    "        \n",
    "        delta_o = (delta_h_k * tanh_c * \n",
    "            self.gate_activator.backward(og))\n",
    "        delta_f = (delta_h_k * og * \n",
    "            (1 - tanh_c * tanh_c) * c_prev *\n",
    "            self.gate_activator.backward(fg))\n",
    "        delta_i = (delta_h_k * og * \n",
    "            (1 - tanh_c * tanh_c) * ct *\n",
    "            self.gate_activator.backward(ig))\n",
    "        delta_ct = (delta_h_k * og * \n",
    "            (1 - tanh_c * tanh_c) * ig *\n",
    "            self.output_activator.backward(ct))\n",
    "        \n",
    "#         print('-----c------',c)\n",
    "#         print('-----ig------',ig)\n",
    "#         print('-----og------',og)\n",
    "#         print('-----fg------',fg)\n",
    "#         print('-----ct------',ct)\n",
    "#         print('-----cp------',c_prev)\n",
    "\n",
    "        \n",
    "        # calc delta_net and delta by formula 9\n",
    "#         delta_o = (delta_h_k * self.output_activator.forward(c) *\n",
    "#                    self.gate_activator.backward(og))\n",
    "#         delta_f = (delta_h_k * og * self.output_activator.backward(c) *\n",
    "#                   c_prev * self.gate_activator.backward(fg))\n",
    "#         delta_i = (delta_h_k * og * self.output_activator.backward(c) *\n",
    "#                   ct * self.gate_activator.backward(ig))\n",
    "#         delta_ct = (delta_h_k * og * self.output_activator.backward(c) *\n",
    "#                   ig * self.output_activator.backward(ct))\n",
    "        # 通过全导数工时得到\n",
    "        delta_h_prev = (np.dot(delta_o.T,self.Woh)+np.dot(delta_i.T,self.Wih)+\n",
    "                       np.dot(delta_f.T,self.Wfh)+np.dot(delta_ct.T,self.Wch)).T\n",
    "        # save and return \n",
    "        self.delta_f_list[k] = delta_f\n",
    "        self.delta_i_list[k] = delta_i\n",
    "        self.delta_o_list[k] = delta_o\n",
    "        self.delta_ct_list[k] = delta_ct\n",
    "        self.delta_h_list[k-1] = delta_h_prev\n",
    "        return delta_h_prev\n",
    "    \n",
    "    def calc_gradient(self):\n",
    "        \"\"\"calc gradient by add all time steps\n",
    "        1.init\n",
    "        2.calc k steps and sum thems\n",
    "        \"\"\"\n",
    "        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (\n",
    "            self._init_weights_grad())\n",
    "        self.Wih_grad, self.Wix_grad, self.bi_grad = (\n",
    "            self._init_weights_grad())\n",
    "        self.Woh_grad, self.Wox_grad, self.bo_grad = (\n",
    "            self._init_weights_grad())\n",
    "        self.Wch_grad, self.Wcx_grad, self.bc_grad = (\n",
    "            self._init_weights_grad())\n",
    "        \n",
    "        for k in range(self.times,0,-1):\n",
    "            (Wfh_grad, Wfx_grad, bf_grad,\n",
    "            Wih_grad, Wix_grad, bi_grad,\n",
    "            Woh_grad, Wox_grad, bo_grad,\n",
    "            Wch_grad, Wcx_grad, bc_grad) = (\n",
    "                self.calc_gradient_weights_k(k))\n",
    "            # 实际梯度是各时刻梯度之和\n",
    "#             print(Wfx_grad\n",
    "#             , bf_grad\n",
    "#             ,Wix_grad\n",
    "#             ,bi_grad\n",
    "#             ,Wox_grad\n",
    "#             ,bo_grad\n",
    "#             ,Wcx_grad\n",
    "#             ,bc_grad)\n",
    "            \n",
    "            self.Wfh_grad += Wfh_grad\n",
    "            self.Wfx_grad += Wfx_grad\n",
    "            self.bf_grad += bf_grad\n",
    "            self.Wih_grad += Wih_grad\n",
    "            self.Wix_grad += Wix_grad\n",
    "            self.bi_grad += bi_grad\n",
    "            self.Woh_grad += Woh_grad\n",
    "            self.Wox_grad += Wox_grad\n",
    "            self.bo_grad += bo_grad\n",
    "            self.Wch_grad += Wch_grad\n",
    "            self.Wcx_grad += Wcx_grad\n",
    "            self.bc_grad += bc_grad\n",
    "\n",
    "        \n",
    "    def calc_gradient_weights_k(self,k):\n",
    "        \"\"\"calc k step gradient\n",
    "        return 4*3 matrix (every element is matrix gradient)\n",
    "                1.get delta_h\n",
    "        2.get x\n",
    "        3.calc w_*h b grad\n",
    "        4.calc w_*x grad\n",
    "        \"\"\"\n",
    "        \n",
    "#         print(k)\n",
    "        x = self.input_list[k]\n",
    "        h_prev_t = self.h_list[k-1].T\n",
    "#         print(h_prev_t)\n",
    "#         print(x)\n",
    "#         print(self.delta_f_list[k])\n",
    "        wfh_grad = np.dot(self.delta_f_list[k],h_prev_t)\n",
    "        wih_grad = np.dot(self.delta_i_list[k],h_prev_t)\n",
    "        woh_grad = np.dot(self.delta_o_list[k],h_prev_t)\n",
    "        wch_grad = np.dot(self.delta_ct_list[k],h_prev_t)\n",
    "        \n",
    "        x_t = x.T\n",
    "        wfx_grad = np.dot(self.delta_f_list[k],x_t)\n",
    "        wix_grad = np.dot(self.delta_i_list[k],x_t)\n",
    "        wox_grad = np.dot(self.delta_o_list[k],x_t)\n",
    "        wcx_grad = np.dot(self.delta_ct_list[k],x_t)\n",
    "        \n",
    "        bf_grad = self.delta_f_list[k]\n",
    "        bi_grad = self.delta_i_list[k]\n",
    "        bo_grad = self.delta_o_list[k]\n",
    "        bc_grad = self.delta_ct_list[k]\n",
    "        \n",
    "        return (wfh_grad,wfx_grad,bf_grad,\n",
    "                wih_grad,wix_grad,bi_grad,\n",
    "                woh_grad,wox_grad,bo_grad,\n",
    "                wch_grad,wcx_grad,bc_grad)\n",
    "        \n",
    "    \n",
    "    def update_params(self):\n",
    "        \"\"\"update params with sgd + l2 regularization\n",
    "        \"\"\"\n",
    "        grads = [\n",
    "            self.Wfh_grad,\n",
    "            self.Wfx_grad,\n",
    "            self.bf_grad,\n",
    "            self.Wih_grad,\n",
    "            self.Wix_grad,\n",
    "            self.bi_grad,\n",
    "            self.Woh_grad,\n",
    "            self.Wox_grad,\n",
    "            self.bo_grad,\n",
    "            self.Wch_grad,\n",
    "            self.Wcx_grad,\n",
    "            self.bc_grad\n",
    "        ]\n",
    "        params = [\n",
    "            self.Wfh,\n",
    "            self.Wfx,\n",
    "            self.bf,\n",
    "            self.Wih,\n",
    "            self.Wix,\n",
    "            self.bi,\n",
    "            self.Woh,\n",
    "            self.Wox,\n",
    "            self.bo,\n",
    "            self.Wch,\n",
    "            self.Wcx,\n",
    "            self.bc\n",
    "        ]\n",
    "        \n",
    "        for i,param in enumerate(params):\n",
    "            param[...] *= (1-self.weight_decay)\n",
    "            param[...] -= self.learning_rate*grads[i]\n",
    "            # 完成对于本地属性的修改 而不是引用的修改\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"for gradient check\"\"\"\n",
    "        self.times = 0\n",
    "        self.c_list = self._init_state_vec_list()\n",
    "        self.h_list = self._init_state_vec_list()\n",
    "        self.f_list = self._init_state_vec_list()\n",
    "        self.i_list = self._init_state_vec_list()\n",
    "        self.o_list = self._init_state_vec_list()\n",
    "        self.ct_list = self._init_state_vec_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set():\n",
    "    data = [np.array([[1],[2],[3],[4]]),\n",
    "            np.array([[2],[3],[4],[5]]),\n",
    "            np.array([[2],[3],[4],[5]]),\n",
    "            np.array([[2],[3],[4],[5]]),\n",
    "           ]\n",
    "    label = np.array([[1],[2],[3]])\n",
    "    return data,label\n",
    "        \n",
    "def gradient_check():\n",
    "    \"\"\"gradient check \n",
    "    by taking sum function as loss_func to guarantee delta_T = 1\n",
    "    by using identity activator to guarantee easy computation of bp\n",
    "    \"\"\"\n",
    "    loss_func = lambda x : x.sum()\n",
    "\n",
    "    lstmcell = LstmLayer(4,3,1e-3,1e-2)\n",
    "    # forward\n",
    "    d,l = data_set()\n",
    "    for i in range(len(d)):\n",
    "        lstmcell.forward(d[i])\n",
    "    # get delta_T\n",
    "    delta = np.ones(lstmcell.h_list[-1].shape,dtype=np.float32)\n",
    "    # bp\n",
    "    lstmcell.backward(delta,IdentityActivator())\n",
    "    \n",
    "    # gradient_check\n",
    "    epsilon = 1e-4\n",
    "    lstm_params = [ lstmcell.Wfh, lstmcell.Wfx, lstmcell.bf,\n",
    "                    lstmcell.Wih, lstmcell.Wix, lstmcell.bi,\n",
    "                    lstmcell.Woh, lstmcell.Wox, lstmcell.bo,\n",
    "                    lstmcell.Wch, lstmcell.Wcx, lstmcell.bc]\n",
    "    lstm_params_grads = grads = [\n",
    "            lstmcell.Wfh_grad,lstmcell.Wfx_grad,lstmcell.bf_grad,\n",
    "            lstmcell.Wih_grad,lstmcell.Wix_grad,lstmcell.bi_grad,\n",
    "            lstmcell.Woh_grad,lstmcell.Wox_grad,lstmcell.bo_grad,\n",
    "            lstmcell.Wch_grad,lstmcell.Wcx_grad,lstmcell.bc_grad]\n",
    "    param_names = [ 'Wfh', 'Wfx', 'bf',\n",
    "                    'Wih', 'Wix', 'bi',\n",
    "                    'Woh', 'Wox', 'bo',\n",
    "                    'Wch', 'Wcx', 'bc']\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    for k in range(len(lstm_params)):\n",
    "        for i in range(lstm_params[k].shape[0]):\n",
    "            for j in range(lstm_params[k].shape[1]):\n",
    "                lstm_params[k][i,j] += epsilon\n",
    "                lstmcell.reset_state()\n",
    "                for z in range(len(d)):\n",
    "                    lstmcell.forward(d[z])\n",
    "                loss1 = loss_func(lstmcell.h_list[-1])\n",
    "                lstm_params[k][i,j] -= 2*epsilon\n",
    "                lstmcell.reset_state()\n",
    "                for z in range(len(d)):\n",
    "                    lstmcell.forward(d[z])\n",
    "                loss2 = loss_func(lstmcell.h_list[-1])\n",
    "                expected_grad = (loss1-loss2) / (2*epsilon)\n",
    "                # 还原保证其他计算正确性\n",
    "                lstm_params[k][i,j] += epsilon\n",
    "                print('%s(%d,%d):expected - actual_val = %.5e - %.5e' % (\n",
    "                    param_names[k],i,j,expected_grad,lstm_params_grads[k][i,j]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 10]\n"
     ]
    }
   ],
   "source": [
    "a0 = [1,2,3]\n",
    "a1 = [4,5,6]\n",
    "a = [a0,a1]\n",
    "a[0][2] = 10\n",
    "print(a0)\n",
    "# 通过索引确实可以实现修改 所以想要修改引用元素的本身不能使用= 需要使用索引赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wfh(0,0):expected - actual_val = 1.94696e-04 - 1.41176e-04\n",
      "Wfh(0,1):expected - actual_val = 2.12967e-04 - 1.55197e-04\n",
      "Wfh(0,2):expected - actual_val = 7.40179e-05 - 5.41166e-05\n",
      "Wfh(1,0):expected - actual_val = 2.13551e-04 - 1.55093e-04\n",
      "Wfh(1,1):expected - actual_val = 2.33613e-04 - 1.70498e-04\n",
      "Wfh(1,2):expected - actual_val = 8.11978e-05 - 5.94527e-05\n",
      "Wfh(2,0):expected - actual_val = 7.41659e-05 - 5.42819e-05\n",
      "Wfh(2,1):expected - actual_val = 8.11447e-05 - 5.96718e-05\n",
      "Wfh(2,2):expected - actual_val = 2.82060e-05 - 2.08071e-05\n",
      "Wfx(0,0):expected - actual_val = -1.78838e-02 - -1.18677e-02\n",
      "Wfx(0,1):expected - actual_val = -2.68258e-02 - -1.78016e-02\n",
      "Wfx(0,2):expected - actual_val = -3.57677e-02 - -2.37354e-02\n",
      "Wfx(0,3):expected - actual_val = -4.47096e-02 - -2.96693e-02\n",
      "Wfx(1,0):expected - actual_val = -1.95852e-02 - -1.30351e-02\n",
      "Wfx(1,1):expected - actual_val = -2.93777e-02 - -1.95526e-02\n",
      "Wfx(1,2):expected - actual_val = -3.91703e-02 - -2.60702e-02\n",
      "Wfx(1,3):expected - actual_val = -4.89629e-02 - -3.25877e-02\n",
      "Wfx(2,0):expected - actual_val = -6.78615e-03 - -4.56474e-03\n",
      "Wfx(2,1):expected - actual_val = -1.01792e-02 - -6.84710e-03\n",
      "Wfx(2,2):expected - actual_val = -1.35723e-02 - -9.12947e-03\n",
      "Wfx(2,3):expected - actual_val = -1.69654e-02 - -1.14118e-02\n",
      "bf(0,0):expected - actual_val = -8.94192e-03 - -5.93386e-03\n",
      "bf(1,0):expected - actual_val = -9.79258e-03 - -6.51754e-03\n",
      "bf(2,0):expected - actual_val = -3.39307e-03 - -2.28237e-03\n",
      "Wih(0,0):expected - actual_val = 2.59300e-04 - 1.72024e-04\n",
      "Wih(0,1):expected - actual_val = 2.82913e-04 - 1.89108e-04\n",
      "Wih(0,2):expected - actual_val = 9.81846e-05 - 6.59413e-05\n",
      "Wih(1,0):expected - actual_val = 2.75689e-04 - 1.82172e-04\n",
      "Wih(1,1):expected - actual_val = 3.00775e-04 - 2.00267e-04\n",
      "Wih(1,2):expected - actual_val = 1.04380e-04 - 6.98330e-05\n",
      "Wih(2,0):expected - actual_val = 1.01301e-04 - 6.73369e-05\n",
      "Wih(2,1):expected - actual_val = 1.10526e-04 - 7.40227e-05\n",
      "Wih(2,2):expected - actual_val = 3.83581e-05 - 2.58110e-05\n",
      "Wix(0,0):expected - actual_val = -2.54686e-02 - -1.44612e-02\n",
      "Wix(0,1):expected - actual_val = -3.85176e-02 - -2.16918e-02\n",
      "Wix(0,2):expected - actual_val = -5.15666e-02 - -2.89223e-02\n",
      "Wix(0,3):expected - actual_val = -6.46156e-02 - -3.61529e-02\n",
      "Wix(1,0):expected - actual_val = -2.70837e-02 - -1.53106e-02\n",
      "Wix(1,1):expected - actual_val = -4.09499e-02 - -2.29659e-02\n",
      "Wix(1,2):expected - actual_val = -5.48160e-02 - -3.06212e-02\n",
      "Wix(1,3):expected - actual_val = -6.86821e-02 - -3.82765e-02\n",
      "Wix(2,0):expected - actual_val = -9.92257e-03 - -5.66316e-03\n",
      "Wix(2,1):expected - actual_val = -1.49933e-02 - -8.49474e-03\n",
      "Wix(2,2):expected - actual_val = -2.00640e-02 - -1.13263e-02\n",
      "Wix(2,3):expected - actual_val = -2.51348e-02 - -1.41579e-02\n",
      "bi(0,0):expected - actual_val = -1.30490e-02 - -7.23059e-03\n",
      "bi(1,0):expected - actual_val = -1.38661e-02 - -7.65529e-03\n",
      "bi(2,0):expected - actual_val = -5.07074e-03 - -2.83158e-03\n",
      "Woh(0,0):expected - actual_val = 3.05963e-04 - 3.05926e-04\n",
      "Woh(0,1):expected - actual_val = 3.36348e-04 - 3.36309e-04\n",
      "Woh(0,2):expected - actual_val = 1.17283e-04 - 1.17270e-04\n",
      "Woh(1,0):expected - actual_val = 3.25597e-04 - 3.25645e-04\n",
      "Woh(1,1):expected - actual_val = 3.57942e-04 - 3.57990e-04\n",
      "Woh(1,2):expected - actual_val = 1.24815e-04 - 1.24831e-04\n",
      "Woh(2,0):expected - actual_val = 1.21763e-04 - 1.21697e-04\n",
      "Woh(2,1):expected - actual_val = 1.33848e-04 - 1.33781e-04\n",
      "Woh(2,2):expected - actual_val = 4.66708e-05 - 4.66482e-05\n",
      "Wox(0,0):expected - actual_val = -2.57251e-02 - -2.57175e-02\n",
      "Wox(0,1):expected - actual_val = -3.85881e-02 - -3.85762e-02\n",
      "Wox(0,2):expected - actual_val = -5.14511e-02 - -5.14350e-02\n",
      "Wox(0,3):expected - actual_val = -6.43141e-02 - -6.42937e-02\n",
      "Wox(1,0):expected - actual_val = -2.73593e-02 - -2.73690e-02\n",
      "Wox(1,1):expected - actual_val = -4.10384e-02 - -4.10535e-02\n",
      "Wox(1,2):expected - actual_val = -5.47174e-02 - -5.47380e-02\n",
      "Wox(1,3):expected - actual_val = -6.83965e-02 - -6.84225e-02\n",
      "Wox(2,0):expected - actual_val = -1.02479e-02 - -1.02345e-02\n",
      "Wox(2,1):expected - actual_val = -1.53725e-02 - -1.53518e-02\n",
      "Wox(2,2):expected - actual_val = -2.04972e-02 - -2.04690e-02\n",
      "Wox(2,3):expected - actual_val = -2.56219e-02 - -2.55863e-02\n",
      "bo(0,0):expected - actual_val = -1.28630e-02 - -1.28587e-02\n",
      "bo(1,0):expected - actual_val = -1.36791e-02 - -1.36845e-02\n",
      "bo(2,0):expected - actual_val = -5.12467e-03 - -5.11726e-03\n",
      "Wch(0,0):expected - actual_val = -8.83999e-03 - -5.86512e-03\n",
      "Wch(0,1):expected - actual_val = -9.64502e-03 - -6.44762e-03\n",
      "Wch(0,2):expected - actual_val = -3.34730e-03 - -2.24826e-03\n",
      "Wch(1,0):expected - actual_val = -9.51443e-03 - -6.28736e-03\n",
      "Wch(1,1):expected - actual_val = -1.03802e-02 - -6.91187e-03\n",
      "Wch(1,2):expected - actual_val = -3.60232e-03 - -2.41017e-03\n",
      "Wch(2,0):expected - actual_val = -8.64416e-03 - -5.74156e-03\n",
      "Wch(2,1):expected - actual_val = -9.43115e-03 - -6.31164e-03\n",
      "Wch(2,2):expected - actual_val = -3.27304e-03 - -2.20081e-03\n",
      "Wcx(0,0):expected - actual_val = 8.74848e-01 - 4.93052e-01\n",
      "Wcx(0,1):expected - actual_val = 1.32631e+00 - 7.39578e-01\n",
      "Wcx(0,2):expected - actual_val = 1.77777e+00 - 9.86104e-01\n",
      "Wcx(0,3):expected - actual_val = 2.22922e+00 - 1.23263e+00\n",
      "Wcx(1,0):expected - actual_val = 9.42821e-01 - 5.28419e-01\n",
      "Wcx(1,1):expected - actual_val = 1.42949e+00 - 7.92629e-01\n",
      "Wcx(1,2):expected - actual_val = 1.91617e+00 - 1.05684e+00\n",
      "Wcx(1,3):expected - actual_val = 2.40284e+00 - 1.32105e+00\n",
      "Wcx(2,0):expected - actual_val = 8.55937e-01 - 4.82877e-01\n",
      "Wcx(2,1):expected - actual_val = 1.29772e+00 - 7.24315e-01\n",
      "Wcx(2,2):expected - actual_val = 1.73950e+00 - 9.65754e-01\n",
      "Wcx(2,3):expected - actual_val = 2.18129e+00 - 1.20719e+00\n",
      "bc(0,0):expected - actual_val = 4.51459e-01 - 2.46526e-01\n",
      "bc(1,0):expected - actual_val = 4.86673e-01 - 2.64210e-01\n",
      "bc(2,0):expected - actual_val = 4.41784e-01 - 2.41438e-01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 纠错\n",
    "- 通过gradient check 发现 -》 对于权重的梯度公式有小问题\n",
    "- 问题是：\n",
    "    - 对于wh的梯度 在k=1时刻 由于h0 = 0所以这是贡献的梯度=0\n",
    "    - 对于bias 以及 wx 在k=1时刻 由于对于bias恒为1 而x!=0所以贡献的梯度不为0\n",
    "    - 因此需要对于初始化列表中h_list = [0,...] x_list = [...]开始没有0,但是为了程序方便，在x_list中预先添加0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "- 通过gradient check 发现对于o的 woh wox bo都是正确的\n",
    "- 但是对于i f ct的梯度的计算只有第一次是正确的 后面的得到的结果太小，导致后面的没有生效，所以累加的结果偏小，但是从公式推导的角度来看没有发现存在的问题，这个bug整整调了一天，最终也没有合适的解决方案，有待解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
