{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lesson2 - hands on implementation RNN ------ basic RNNcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入 **语言模型LM**\n",
    "- 定义：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。\n",
    "- 应用：STT语音转文本  OCR图像转文本\n",
    "- 过去的模型：N-Gram\n",
    "    - 假设一个词出现的概率只与前面的N个词有关 模型的大小和N的关系是指数级的\n",
    "- 现在的模型：RNN循环神经网络 理论上可以往前看(往后看)任意多个词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "- ![](http://upload-images.jianshu.io/upload_images/2256672-cf18bb1f06e750a4.jpg)\n",
    "- formula $$\\begin{align}\n",
    "\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\qquad\\qquad\\quad(式1)\\\\\n",
    "\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\qquad(式2)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络升级\n",
    "- 以语言模型需求进行分析 发现光是查看之前的word不足够\n",
    "### 双向循环神经网络 Bidirectional RNN \n",
    "- ![](http://upload-images.jianshu.io/upload_images/2256672-039a45251aa5d220.png)\n",
    "- formula:$$\\begin{align}\n",
    "\\mathrm{o}_t&=g(V\\mathrm{s}_t+V'\\mathrm{s}_t')\\\\\n",
    "\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\\n",
    "\\mathrm{s}_t'&=f(U'\\mathrm{x}_t+W'\\mathrm{s}_{t+1}')\\\\\n",
    "\\end{align}$$\n",
    "- 正向计算和反向计算不共享权重，也就是说U和U'、W和W'、V和V'都是不同的权重矩阵\n",
    "### 深度循环神经网络\n",
    "- ![](http://upload-images.jianshu.io/upload_images/2256672-df137de8007c3d26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480)\n",
    "- formula:$$\\begin{align}\n",
    "\\mathrm{o}_t&=g(V^{(i)}\\mathrm{s}_t^{(i)}+V'^{(i)}\\mathrm{s}_t'^{(i)})\\\\\n",
    "\\mathrm{s}_t^{(i)}&=f(U^{(i)}\\mathrm{s}_t^{(i-1)}+W^{(i)}\\mathrm{s}_{t-1})\\\\\n",
    "\\mathrm{s}_t'^{(i)}&=f(U'^{(i)}\\mathrm{s}_t'^{(i-1)}+W'^{(i)}\\mathrm{s}_{t+1}')\\\\\n",
    "...\\\\\n",
    "\\mathrm{s}_t^{(1)}&=f(U^{(1)}\\mathrm{x}_t+W^{(1)}\\mathrm{s}_{t-1})\\\\\n",
    "\\mathrm{s}_t'^{(1)}&=f(U'^{(1)}\\mathrm{x}_t+W'^{(1)}\\mathrm{s}_{t+1}')\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心：BPTT 沿着时间反向传播\n",
    "1. 前向计算每个神经元的输出值；\n",
    "2. 反向计算每个神经元的误差项值，它是误差函数E对神经元j的加权输入的偏导数；\n",
    "3. 计算每个权重的梯度。\n",
    "\n",
    "### 反向传播\n",
    "- 误差项的传递\n",
    "    - 一个方向是其传递到上一层网络，这部分只和权重矩阵U有关\n",
    "    - 另一个是方向是将其沿时间线传递到初始时刻得到，这部分只和权重矩阵W有关\n",
    "- 对于权重的求导\n",
    "- 数学知识补充：\n",
    "    - 向量函数对向量求导，其结果为Jacobian矩阵 \n",
    "    - diag[a]表示根据向量a创建一个对角矩阵 $$ diag(\\mathrm{a})=\\begin{bmatrix}\n",
    "a_1 & 0 & ... & 0\\\\\n",
    "0 & a_2 & ... & 0\\\\\n",
    "&.\\\\&.\\\\\n",
    "0 & 0 & ... & a_n\\\\\n",
    "\\end{bmatrix}\\\\ $$\n",
    "\n",
    "#### 沿着时间反向传播\n",
    "- $$\\begin{align}\n",
    "\\frac{\\partial{\\mathrm{net}_t}}{\\partial{\\mathrm{s}_{t-1}}}&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{net_1^t}}{\\partial{s_1^{t-1}}}& \\frac{\\partial{net_1^t}}{\\partial{s_2^{t-1}}}& ...&  \\frac{\\partial{net_1^t}}{\\partial{s_n^{t-1}}}\\\\\n",
    "\\frac{\\partial{net_2^t}}{\\partial{s_1^{t-1}}}& \\frac{\\partial{net_2^t}}{\\partial{s_2^{t-1}}}& ...&  \\frac{\\partial{net_2^t}}{\\partial{s_n^{t-1}}}\\\\\n",
    "&.\\\\&.\\\\\n",
    "\\frac{\\partial{net_n^t}}{\\partial{s_1^{t-1}}}& \\frac{\\partial{net_n^t}}{\\partial{s_2^{t-1}}}& ...&  \\frac{\\partial{net_n^t}}{\\partial{s_n^{t-1}}}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "w_{11} & w_{12} & ... & w_{1n}\\\\\n",
    "w_{21} & w_{22} & ... & w_{2n}\\\\\n",
    "&.\\\\&.\\\\\n",
    "w_{n1} & w_{n2} & ... & w_{nn}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=W\n",
    "\\end{align}$$ \n",
    "- $$\\begin{align}\n",
    "\\frac{\\partial{\\mathrm{s}_{t-1}}}{\\partial{\\mathrm{net}_{t-1}}}&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{s_1^{t-1}}}{\\partial{net_1^{t-1}}}& \\frac{\\partial{s_1^{t-1}}}{\\partial{net_2^{t-1}}}& ...&  \\frac{\\partial{s_1^{t-1}}}{\\partial{net_n^{t-1}}}\\\\\n",
    "\\frac{\\partial{s_2^{t-1}}}{\\partial{net_1^{t-1}}}& \\frac{\\partial{s_2^{t-1}}}{\\partial{net_2^{t-1}}}& ...&  \\frac{\\partial{s_2^{t-1}}}{\\partial{net_n^{t-1}}}\\\\\n",
    "&.\\\\&.\\\\\n",
    "\\frac{\\partial{s_n^{t-1}}}{\\partial{net_1^{t-1}}}& \\frac{\\partial{s_n^{t-1}}}{\\partial{net_2^{t-1}}}& ...&  \\frac{\\partial{s_n^{t-1}}}{\\partial{net_n^{t-1}}}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "f'(net_1^{t-1}) & 0 & ... & 0\\\\\n",
    "0 & f'(net_2^{t-1}) & ... & 0\\\\\n",
    "&.\\\\&.\\\\\n",
    "0 & 0 & ... & f'(net_n^{t-1})\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=diag[f'(\\mathrm{net}_{t-1})]\n",
    "\\end{align}$$\n",
    "- $$\\begin{align}\n",
    "\\frac{\\partial{\\mathrm{net}_t}}{\\partial{\\mathrm{net}_{t-1}}}&=\\frac{\\partial{\\mathrm{net}_t}}{\\partial{\\mathrm{s}_{t-1}}}\\frac{\\partial{\\mathrm{s}_{t-1}}}{\\partial{\\mathrm{net}_{t-1}}}\\\\\n",
    "&=Wdiag[f'(\\mathrm{net}_{t-1})]\\\\\n",
    "&=\\begin{bmatrix}\n",
    "w_{11}f'(net_1^{t-1}) & w_{12}f'(net_2^{t-1}) & ... & w_{1n}f(net_n^{t-1})\\\\\n",
    "w_{21}f'(net_1^{t-1}) & w_{22} f'(net_2^{t-1}) & ... & w_{2n}f(net_n^{t-1})\\\\\n",
    "&.\\\\&.\\\\\n",
    "w_{n1}f'(net_1^{t-1}) & w_{n2} f'(net_2^{t-1}) & ... & w_{nn} f'(net_n^{t-1})\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}$$\n",
    "- 因此有了向前传播多个时间单位的公式 $$\\begin{align}\n",
    "\\delta_k^T=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_k}}\\\\\n",
    "=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_t}}\\frac{\\partial{\\mathrm{net}_t}}{\\partial{\\mathrm{net}_k}}\\\\\n",
    "=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_t}}\\frac{\\partial{\\mathrm{net}_t}}{\\partial{\\mathrm{net}_{t-1}}}\\frac{\\partial{\\mathrm{net}_{t-1}}}{\\partial{\\mathrm{net}_{t-2}}}...\\frac{\\partial{\\mathrm{net}_{k+1}}}{\\partial{\\mathrm{net}_{k}}}\\\\\n",
    "=&Wdiag[f'(\\mathrm{net}_{t-1})]\n",
    "Wdiag[f'(\\mathrm{net}_{t-2})]\n",
    "...\n",
    "Wdiag[f'(\\mathrm{net}_{k})]\n",
    "\\delta_t^l\\\\\n",
    "=&\\delta_t^T\\prod_{i=k}^{t-1}Wdiag[f'(\\mathrm{net}_{i})]\\qquad(式3)\n",
    "\\end{align}$$\n",
    "\n",
    "#### 沿着层次反向传播\n",
    "- 与全链接神经网络类似 \n",
    "- $$\\begin{align}\n",
    "\\mathrm{net}_t^l=&U\\mathrm{a}_t^{l-1}+W\\mathrm{s}_{t-1}\\\\\n",
    "\\mathrm{a}_t^{l-1}=&f^{l-1}(\\mathrm{net}_t^{l-1})\n",
    "\\end{align}$$\n",
    "- $$\\begin{align}\n",
    "(\\delta_t^{l-1})^T=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_t^{l-1}}}\\\\\n",
    "=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_t^l}}\\frac{\\partial{\\mathrm{net}_t^l}}{\\partial{\\mathrm{net}_t^{l-1}}}\\\\\n",
    "=&(\\delta_t^l)^TUdiag[f'^{l-1}(\\mathrm{net}_t^{l-1})]\\qquad(式4)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于权重的梯度求解\n",
    "- 关键在于求解U W的梯度 对于V的梯度 不需要沿着时间反向传播 只需要当期那输出向前传播一次就可以得到\n",
    "- ![](http://upload-images.jianshu.io/upload_images/2256672-f7d034c8f05812f7.png)\n",
    "- formula: \n",
    "- 对于w公式\n",
    "    - $$\\begin{align}\n",
    "\\mathrm{net}_t=&U\\mathrm{x}_t+W\\mathrm{s}_{t-1}\\\\\n",
    "\\begin{bmatrix}\n",
    "net_1^t\\\\\n",
    "net_2^t\\\\\n",
    ".\\\\.\\\\\n",
    "net_n^t\\\\\n",
    "\\end{bmatrix}=&U\\mathrm{x}_t+\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & ... & w_{1n}\\\\\n",
    "w_{21} & w_{22} & ... & w_{2n}\\\\\n",
    ".\\\\.\\\\\n",
    "w_{n1} & w_{n2} & ... & w_{nn}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_1^{t-1}\\\\\n",
    "s_2^{t-1}\\\\\n",
    ".\\\\.\\\\\n",
    "s_n^{t-1}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&U\\mathrm{x}_t+\n",
    "\\begin{bmatrix}\n",
    "w_{11}s_1^{t-1}+w_{12}s_2^{t-1}...w_{1n}s_n^{t-1}\\\\\n",
    "w_{21}s_1^{t-1}+w_{22}s_2^{t-1}...w_{2n}s_n^{t-1}\\\\\n",
    ".\\\\.\\\\\n",
    "w_{n1}s_1^{t-1}+w_{n2}s_2^{t-1}...w_{nn}s_n^{t-1}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}$$\n",
    "    - $$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{w_{ji}}}=&\\frac{\\partial{E}}{\\partial{net_j^t}}\\frac{\\partial{net_j^t}}{\\partial{w_{ji}}}\\\\\n",
    "=&\\delta_j^ts_i^{t-1}\n",
    "\\end{align}$$\n",
    "    - $$\\nabla_{W_t}E=\\begin{bmatrix}\n",
    "\\delta_1^ts_1^{t-1} & \\delta_1^ts_2^{t-1} & ... &  \\delta_1^ts_n^{t-1}\\\\\n",
    "\\delta_2^ts_1^{t-1} & \\delta_2^ts_2^{t-1} & ... &  \\delta_2^ts_n^{t-1}\\\\\n",
    ".\\\\.\\\\\n",
    "\\delta_n^ts_1^{t-1} & \\delta_n^ts_2^{t-1} & ... &  \\delta_n^ts_n^{t-1}\\\\\n",
    "\\end{bmatrix}\\qquad(式5)$$\n",
    "    - $$\\begin{align}\n",
    "\\nabla_WE=&\\sum_{i=1}^t\\nabla_{W_i}E\\\\\n",
    "=&\\begin{bmatrix}\n",
    "\\delta_1^ts_1^{t-1} & \\delta_1^ts_2^{t-1} & ... &  \\delta_1^ts_n^{t-1}\\\\\n",
    "\\delta_2^ts_1^{t-1} & \\delta_2^ts_2^{t-1} & ... &  \\delta_2^ts_n^{t-1}\\\\\n",
    ".\\\\.\\\\\n",
    "\\delta_n^ts_1^{t-1} & \\delta_n^ts_2^{t-1} & ... &  \\delta_n^ts_n^{t-1}\\\\\n",
    "\\end{bmatrix}\n",
    "+...+\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^1s_1^0 & \\delta_1^1s_2^0 & ... &  \\delta_1^1s_n^0\\\\\n",
    "\\delta_2^1s_1^0 & \\delta_2^1s_2^0 & ... &  \\delta_2^1s_n^0\\\\\n",
    ".\\\\.\\\\\n",
    "\\delta_n^1s_1^0 & \\delta_n^1s_2^0 & ... &  \\delta_n^1s_n^0\\\\\n",
    "\\end{bmatrix}\\qquad(式6)\n",
    "\\end{align}$$\n",
    "- 类似的有U的公式: \n",
    "    - $$\\nabla_{U_t}E=\\begin{bmatrix}\n",
    "\\delta_1^tx_1^t & \\delta_1^tx_2^t & ... &  \\delta_1^tx_m^t\\\\\n",
    "\\delta_2^tx_1^t & \\delta_2^tx_2^t & ... &  \\delta_2^tx_m^t\\\\\n",
    ".\\\\.\\\\\n",
    "\\delta_n^tx_1^t & \\delta_n^tx_2^t & ... &  \\delta_n^tx_m^t\\\\\n",
    "\\end{bmatrix}\\qquad(式8)$$\n",
    "    - $$\\nabla_UE=\\sum_{i=1}^t\\nabla_{U_i}E$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 详解为什么要沿着时间进行求和\n",
    "- 数学基础知识补充\n",
    "    - 矩阵对于矩阵求导 得到4dim tensor\n",
    "    - 4dim张量与向量相乘 维度减少一维 同样适用于n-dim张量\n",
    "    - 相乘操作就是tensor每一个元素与向量进行相乘\n",
    "- $$\\begin{align}\n",
    "\\nabla_WE=&\\frac{\\partial{E}}{\\partial{W}}\\\\\n",
    "=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_t}}\\frac{\\partial{\\mathrm{net}_t}}{\\partial{W}}\\\\\n",
    "=&\\delta_t^T\\frac{\\partial{W}}{\\partial{W}}f(\\mathrm{net}_{t-1})+ \\delta_t^TW\\frac{\\partial{f(\\mathrm{net}_{t-1})}}{\\partial{W}}\\qquad(式7)\\\\\n",
    "\\end{align}$$\n",
    "- 左边：**矩阵对矩阵求导**，其结果是一个**四维张量** $$\\begin{align}\n",
    "\\frac{\\partial{W}}{\\partial{W}}=&\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{w_{11}}}{\\partial{W}} & \\frac{\\partial{w_{12}}}{\\partial{W}} & ... & \\frac{\\partial{w_{1n}}}{\\partial{W}}\\\\\n",
    "\\frac{\\partial{w_{21}}}{\\partial{W}} & \\frac{\\partial{w_{22}}}{\\partial{W}} & ... & \\frac{\\partial{w_{2n}}}{\\partial{W}}\\\\\n",
    ".\\\\.\\\\\n",
    "\\frac{\\partial{w_{n1}}}{\\partial{W}} & \\frac{\\partial{w_{n2}}}{\\partial{W}} & ... & \\frac{\\partial{w_{nn}}}{\\partial{W}}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{w_{11}}}{\\partial{w_{11}}} & \\frac{\\partial{w_{11}}}{\\partial{w_{12}}} & ... & \\frac{\\partial{w_{11}}}{\\partial{_{1n}}}\\\\\n",
    "\\frac{\\partial{w_{11}}}{\\partial{w_{21}}} & \\frac{\\partial{w_{11}}}{\\partial{w_{22}}} & ... & \\frac{\\partial{w_{11}}}{\\partial{_{2n}}}\\\\\n",
    ".\\\\.\\\\\n",
    "\\frac{\\partial{w_{11}}}{\\partial{w_{n1}}} & \\frac{\\partial{w_{11}}}{\\partial{w_{n2}}} & ... & \\frac{\\partial{w_{11}}}{\\partial{_{nn}}}\\\\\n",
    "\\end{bmatrix} &\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{w_{12}}}{\\partial{w_{11}}} & \\frac{\\partial{w_{12}}}{\\partial{w_{12}}} & ... & \\frac{\\partial{w_{12}}}{\\partial{_{1n}}}\\\\\n",
    "\\frac{\\partial{w_{12}}}{\\partial{w_{21}}} & \\frac{\\partial{w_{12}}}{\\partial{w_{22}}} & ... & \\frac{\\partial{w_{12}}}{\\partial{_{2n}}}\\\\\n",
    ".\\\\.\\\\\n",
    "\\frac{\\partial{w_{12}}}{\\partial{w_{n1}}} & \\frac{\\partial{w_{12}}}{\\partial{w_{n2}}} & ... & \\frac{\\partial{w_{12}}}{\\partial{_{nn}}}\\\\\n",
    "\\end{bmatrix}&...\\\\\n",
    ".\\\\.\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & ... & 0\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    ".\\\\.\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    "\\end{bmatrix} &\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & ... & 0\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    ".\\\\.\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    "\\end{bmatrix}&...\\\\\n",
    ".\\\\.\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}$$\n",
    "- $$\\begin{align}\n",
    "\\delta_t^T\\frac{\\partial{W}}{\\partial{W}}f({\\mathrm{net}_{t-1}})=&\n",
    "\\delta_t^T\\frac{\\partial{W}}{\\partial{W}}{\\mathrm{s}_{t-1}}\\\\\n",
    "=&\\delta_t^T\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & ... & 0\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    ".\\\\.\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    "\\end{bmatrix} &\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & ... & 0\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    ".\\\\.\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    "\\end{bmatrix}&...\\\\\n",
    ".\\\\.\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_1^{t-1}\\\\\n",
    "s_2^{t-1}\\\\\n",
    ".\\\\.\\\\\n",
    "s_n^{t-1}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&\\delta_t^T\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_1^{t-1}\\\\\n",
    "0\\\\\n",
    ".\\\\.\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} &\n",
    "\\begin{bmatrix}\n",
    "s_2^{t-1}\\\\\n",
    "0\\\\\n",
    ".\\\\.\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}&...\\\\\n",
    ".\\\\.\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^t & \\delta_2^t & ... &\\delta_n^t\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_1^{t-1}\\\\\n",
    "0\\\\\n",
    ".\\\\.\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} &\n",
    "\\begin{bmatrix}\n",
    "s_2^{t-1}\\\\\n",
    "0\\\\\n",
    ".\\\\.\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}&...\\\\\n",
    ".\\\\.\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^ts_1^{t-1} & \\delta_1^ts_2^{t-1} & ... &  \\delta_1^ts_n^{t-1}\\\\\n",
    "\\delta_2^ts_1^{t-1} & \\delta_2^ts_2^{t-1} & ... &  \\delta_2^ts_n^{t-1}\\\\\n",
    ".\\\\.\\\\\n",
    "\\delta_n^ts_1^{t-1} & \\delta_n^ts_2^{t-1} & ... &  \\delta_n^ts_n^{t-1}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "=&\\nabla_{Wt}E\n",
    "\\end{align}$$\n",
    "- 右边：$$\\begin{align}\n",
    "\\delta_t^TW\\frac{\\partial{f(\\mathrm{net}_{t-1})}}{\\partial{W}}=&\n",
    "\\delta_t^TW\\frac{\\partial{f(\\mathrm{net}_{t-1})}}{\\partial{\\mathrm{net}_{t-1}}}\\frac{\\partial{\\mathrm{net}_{t-1}}}{\\partial{W}}\\\\\n",
    "=&\\delta_t^TWf'(\\mathrm{net}_{t-1})\\frac{\\partial{\\mathrm{net}_{t-1}}}{\\partial{W}}\\\\\n",
    "=&\\delta_t^T\\frac{\\partial{\\mathrm{net}_t}}{\\partial{\\mathrm{net}_{t-1}}}\\frac{\\partial{\\mathrm{net}_{t-1}}}{\\partial{W}}\\\\\n",
    "=&\\delta_{t-1}^T\\frac{\\partial{\\mathrm{net}_{t-1}}}{\\partial{W}}\\\\\n",
    "\\end{align}$$\n",
    "- 最终得到递推公式: $$\\begin{align}\n",
    "\\nabla_WE=&\\frac{\\partial{E}}{\\partial{W}}\\\\\n",
    "=&\\frac{\\partial{E}}{\\partial{\\mathrm{net}_t}}\\frac{\\partial{\\mathrm{net}_t}}{\\partial{W}}\\\\\n",
    "=&\\nabla_{Wt}E+\\delta_{t-1}^T\\frac{\\partial{\\mathrm{net}_{t-1}}}{\\partial{W}}\\\\\n",
    "=&\\nabla_{Wt}E+\\nabla_{Wt-1}E+\\delta_{t-2}^T\\frac{\\partial{\\mathrm{net}_{t-2}}}{\\partial{W}}\\\\\n",
    "=&\\nabla_{Wt}E+\\nabla_{Wt-1}E+...+\\nabla_{W1}E\\\\\n",
    "=&\\sum_{k=1}^t\\nabla_{Wk}E\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN 问题所在 -- gradient explosion and vanishing\n",
    "- RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。\n",
    "- 原因解释: $$\\begin{align}\n",
    "\\delta_k^T=&\\delta_t^T\\prod_{i=k}^{t-1}Wdiag[f'(\\mathrm{net}_{i})]\\\\\n",
    "\\|\\delta_k^T\\|\\leqslant&\\|\\delta_t^T\\|\\prod_{i=k}^{t-1}\\|W\\|\\|diag[f'(\\mathrm{net}_{i})]\\|\\\\\n",
    "\\leqslant&\\|\\delta_t^T\\|(\\beta_W\\beta_f)^{t-k}\n",
    "\\end{align}$$\n",
    "- 上式beta的定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于beta大于1还是小于1）\n",
    "### 问题解决\n",
    "- 对于梯度爆炸问题：通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。\n",
    "- 对于梯度弥散(消失)问题：更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：\n",
    "    1. 合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。\n",
    "    2. 使用relu代替sigmoid和tanh作为激活函数。\n",
    "    3. 使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。我们将在以后的文章中介绍这两种网络。\n",
    "- 综上 这些解决方法的本质就是**底数部分**变成**1**或者将**指数部分**次数减为**低次/0**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于RNN的序列型问题\n",
    "- 输入是词，对输入进行向量化 one-hot / embedding ->(sparse / dense)\n",
    "- **sparse->dense**:one-hot得到了一个高维、稀疏的向量（稀疏是指绝大部分元素的值都是0）。处理这样的向量会导致我们的神经网络有很多的参数，带来庞大的计算量。因此，往往会需要使用一些降维方法，将高维的稀疏向量转变为低维的稠密向量\n",
    "- 输出是概率分布：语言模型要求的输出是下一个最可能的词，我们可以让循环神经网络计算计算词典中每个词是下一个词的概率，这样，概率最大的词就是下一个最可能的词。因此，神经网络的输出向量也是一个N维向量，向量中的每个元素对应着词典中相应的词是下一个词的概率。(softmax)\n",
    "- model训练过程：\n",
    "    - 获取输入-标签对 (<s\\>+sentense[:-1]-\\>sentense[1:]+<e\\>)\n",
    "    - 向量化方法，对输入x和标签y进行向量化\n",
    "    - 我们使用交叉熵误差函数作为优化目标，对模型进行优化\n",
    "- softmax:$$g(z_i)=\\frac{e^{z_i}}{\\sum_{k}e^{z_k}}$$\n",
    "\n",
    "- cross entopy:$$L(y,o)=-\\frac{1}{N}\\sum_{n\\in{N}}{y_nlogo_n}$$ N是训练样本的个数\n",
    "> cross entropy can make sense in probability problem than MSE ([REDERENCE](https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_wise_op(array,op):\n",
    "    \"\"\"\n",
    "    对于numpy类型data进行element wise 的op函数对应的操作\n",
    "    \"\"\"\n",
    "    for i in np.nditer(array,op_flags=['readwrite']):\n",
    "        i[...] = op(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivator(object):\n",
    "    def forward(self,x):\n",
    "        return 1.0 / (1.0+np.exp(-x))\n",
    "    def backward(self,out):\n",
    "        return out * (1-out)\n",
    "    \n",
    "class TanhActivator(object):\n",
    "    def forward(self,x):\n",
    "        return 2.0 / (1.0+np.exp(-2*x)) - 1.0\n",
    "    def backward(self,out):\n",
    "        return 1 - np.square(out,out)\n",
    "        # return 1 - out*out\n",
    "        \n",
    "class IdentityActivator(object):\n",
    "    \"\"\"\n",
    "    多用于gradient check 中的activation function\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    def backward(self,out):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (Rnn Layer defination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(object):\n",
    "    def __init__(self,input_size,state_size,activator,learning_rate=1e-3,weight_decay=1e-2):\n",
    "        self.input_size = input_size\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.activator = activator\n",
    "        \n",
    "        # 为了各个时刻的运算需要的状态以及时间戳记录\n",
    "        self.state_list = []\n",
    "        self.state_list.append(np.zeros((state_size,1)))\n",
    "        self.input_list = []\n",
    "        \n",
    "        self.times = 0\n",
    "        \n",
    "        # 权重参数 在这里RNN有U W 没有V参数 认为 U*s+W*x\n",
    "        self.U = np.random.uniform(1e-2,1e-2,(state_size,input_size))\n",
    "        self.W = np.random.uniform(1e-2,1e-2,(state_size,state_size))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        完成一轮前向计算\n",
    "        \"\"\"\n",
    "        state = (np.dot(self.U,x)+np.dot(self.W,self.state_list[-1]))\n",
    "        element_wise_op(state,self.activator.forward)\n",
    "        self.state_list.append(state)\n",
    "        self.input_list.append(x)\n",
    "\n",
    "        self.times += 1\n",
    "        return state \n",
    "\n",
    "    def backward(self,delta,activator):\n",
    "        \"\"\"\n",
    "        bptt algorithm \n",
    "        input: delta/sensitivity_array\n",
    "        \"\"\"\n",
    "        self.bp_delta(delta,activator)\n",
    "        self.calc_gradient()\n",
    "        \n",
    "    def bp_delta_k(self,k,activator):\n",
    "        \"\"\"向前传播一次 k+1->k\"\"\"\n",
    "        state = self.state_list[k+1].copy()\n",
    "        element_wise_op(state,activator.backward)\n",
    "        self.delta_list[k] = np.dot(\n",
    "            np.dot(self.delta_list[k+1].T,self.W),\n",
    "            np.diag(state[:,0])).T\n",
    "        \n",
    "        \n",
    "    def bp_delta(self,delta,activator):\n",
    "        \"\"\"从后向前传播 倒着计算出一系列的delta value\"\"\"\n",
    "        self.delta_list = []\n",
    "        for i in range(self.times):\n",
    "            # 每次前向传播 都会time++ 对应产生了一个新的state 以及一个新的delta\n",
    "            # 但是state最开始有一个 delta最开始有0个 因为delta是针对加权输入的 但也可以认为有1个\n",
    "            self.delta_list.append(np.zeros((self.state_size,1)))\n",
    "        self.delta_list.append(delta)\n",
    "        # 向前传播\n",
    "        for i in range(self.times-1,0,-1):\n",
    "            self.bp_delta_k(i,self.activator)\n",
    "            \n",
    "    def calc_gradient_k(self,k):\n",
    "        \"\"\"计算每个时间k的权重的梯度\"\"\"\n",
    "        # 向量外积 outer\n",
    "        print(self.delta_list[k].shape,self.state_list[k-1].shape,self.input_list[k-1].shape)\n",
    "        \n",
    "        gradient_w = np.dot(self.delta_list[k],self.state_list[k-1].T)\n",
    "        gradient_u = np.dot(self.delta_list[k],self.input_list[k-1].T)\n",
    "        self.gradient_w_list[k] = gradient_w\n",
    "        self.gradient_u_list[k] = gradient_u\n",
    "        \n",
    "    def calc_gradient(self):\n",
    "        \"\"\"梯度是各个时刻梯度求和\"\"\"\n",
    "        self.gradient_w_list = []\n",
    "        self.gradient_u_list = []\n",
    "        \n",
    "        for i in range(self.times+1):\n",
    "            self.gradient_w_list.append(np.zeros((self.state_size,self.state_size)))\n",
    "            self.gradient_u_list.append(np.zeros((self.state_size,self.input_size)))\n",
    "        \n",
    "        for i in range(self.times,0,-1):\n",
    "            # 0 步骤 没有权重梯度 没有反向传播的delta_0\n",
    "            self.calc_gradient_k(i)\n",
    "        # 聚合最终结果 累加\n",
    "        from functools import reduce\n",
    "        self.gradient_w = reduce(lambda x,y:x+y,self.gradient_w_list,\n",
    "                              self.gradient_w_list[0])\n",
    "        self.gradient_u = reduce(lambda x,y:x+y,self.gradient_u_list,\n",
    "                              self.gradient_u_list[0])\n",
    "        return (self.gradient_w,self.gradient_u)\n",
    "        \n",
    "    def update_params(self):\n",
    "        \"\"\"sgd with l2 regularization\"\"\"\n",
    "        self.W *= (1-self.weight_decay)\n",
    "        self.U *= (1-self.weight_decay)\n",
    "        self.W -= self.learning_rate * self.gradient_w\n",
    "        self.U -= self.learning_rate * self.gradient_u\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"reset state to make gradient check more easy\"\"\"\n",
    "        self.times = 0\n",
    "        self.state_list = []\n",
    "        self.state_list.append(np.zeros((self.state_size,1)))\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set():\n",
    "    data = [np.array([[1],[2],[3],[4]]),\n",
    "           np.array([[2],[3],[4],[5]]),\n",
    "           np.array([[2],[3],[4],[5]]),\n",
    "           np.array([[2],[3],[4],[5]])]\n",
    "    label = np.array([[1],[2],[3]])\n",
    "    return data,label\n",
    "        \n",
    "def gradient_check():\n",
    "    \"\"\"gradient check \n",
    "    by taking sum function as loss_func to guarantee delta_T = 1\n",
    "    by using identity activator to guarantee easy computation of bp\n",
    "    \"\"\"\n",
    "    loss_func = lambda x : x.sum()\n",
    "\n",
    "    test_rnn = RecurrentLayer(4,3,IdentityActivator(),1e-3,1e-2)\n",
    "    # forward\n",
    "    d,l = data_set()\n",
    "    for i in range(len(d)):\n",
    "        test_rnn.forward(d[i])\n",
    "    # get delta_T 注意类型\n",
    "    delta = np.ones(test_rnn.state_list[-1].shape,dtype=np.float32)\n",
    "    # bp\n",
    "    test_rnn.backward(delta,IdentityActivator())\n",
    "    \n",
    "    # gradient_check\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(test_rnn.W.shape[0]):\n",
    "        for j in range(test_rnn.W.shape[1]):\n",
    "            test_rnn.reset_state()\n",
    "            test_rnn.W[i,j] += epsilon\n",
    "            for z in range(len(d)):\n",
    "                test_rnn.forward(d[z])\n",
    "            loss1 = loss_func(test_rnn.state_list[-1])\n",
    "            test_rnn.W[i,j] -= 2*epsilon\n",
    "            test_rnn.reset_state()\n",
    "            for z in range(len(d)):\n",
    "                test_rnn.forward(d[z])\n",
    "            loss2 = loss_func(test_rnn.state_list[-1])\n",
    "            expected_grad = (loss1-loss2) / (2*epsilon)\n",
    "            # 还原保证其他计算正确性\n",
    "            test_rnn.W[i,j] += epsilon\n",
    "            print('w(%d,%d):expected - actual_val = %.5f - %.5f' % (\n",
    "                i,j,expected_grad,test_rnn.gradient_w[i,j]))\n",
    "    for i in range(test_rnn.U.shape[0]):\n",
    "        for j in range(test_rnn.U.shape[1]):\n",
    "            test_rnn.reset_state()\n",
    "            test_rnn.U[i,j] += epsilon\n",
    "            for z in range(len(d)):\n",
    "                test_rnn.forward(d[z])\n",
    "            loss1 = loss_func(test_rnn.state_list[-1])\n",
    "            test_rnn.U[i,j] -= 2*epsilon\n",
    "            test_rnn.reset_state()\n",
    "            for z in range(len(d)):\n",
    "                test_rnn.forward(d[z])\n",
    "            loss2 = loss_func(test_rnn.state_list[-1])\n",
    "            expected_grad = (loss1-loss2) / (2*epsilon)\n",
    "            # 还原保证其他计算正确性\n",
    "            test_rnn.U[i,j] += epsilon\n",
    "            print('u(%d,%d):expected - actual_val = %.5f - %.5f' % (\n",
    "                i,j,expected_grad,test_rnn.gradient_u[i,j]))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "reduce(function, sequence[, initial]) -> value\n",
       "\n",
       "Apply a function of two arguments cumulatively to the items of a sequence,\n",
       "from left to right, so as to reduce the sequence to a single value.\n",
       "For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n",
       "((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n",
       "of the sequence in the calculation, and serves as a default when the\n",
       "sequence is empty.\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5,1,-1):\n",
    "    print(i)\n",
    "from functools import reduce\n",
    "?reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) (3, 1) (4, 1)\n",
      "(3, 1) (3, 1) (4, 1)\n",
      "(3, 1) (3, 1) (4, 1)\n",
      "(3, 1) (3, 1) (4, 1)\n",
      "w(0,0):expected - actual_val = 0.14867 - 0.14867\n",
      "w(0,1):expected - actual_val = 0.14867 - 0.14867\n",
      "w(0,2):expected - actual_val = 0.14867 - 0.14867\n",
      "w(1,0):expected - actual_val = 0.14867 - 0.14867\n",
      "w(1,1):expected - actual_val = 0.14867 - 0.14867\n",
      "w(1,2):expected - actual_val = 0.14867 - 0.14867\n",
      "w(2,0):expected - actual_val = 0.14867 - 0.14867\n",
      "w(2,1):expected - actual_val = 0.14867 - 0.14867\n",
      "w(2,2):expected - actual_val = 0.14867 - 0.14867\n",
      "u(0,0):expected - actual_val = 2.06183 - 2.06183\n",
      "u(0,1):expected - actual_val = 3.09275 - 3.09275\n",
      "u(0,2):expected - actual_val = 4.12368 - 4.12368\n",
      "u(0,3):expected - actual_val = 5.15461 - 5.15461\n",
      "u(1,0):expected - actual_val = 2.06183 - 2.06183\n",
      "u(1,1):expected - actual_val = 3.09275 - 3.09275\n",
      "u(1,2):expected - actual_val = 4.12368 - 4.12368\n",
      "u(1,3):expected - actual_val = 5.15461 - 5.15461\n",
      "u(2,0):expected - actual_val = 2.06183 - 2.06183\n",
      "u(2,1):expected - actual_val = 3.09275 - 3.09275\n",
      "u(2,2):expected - actual_val = 4.12368 - 4.12368\n",
      "u(2,3):expected - actual_val = 5.15461 - 5.15461\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "if __name__ == '__main__':\n",
    "    gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "[[1, 2], [5, 6]]\n",
      "[2, 3]\n",
      "True\n",
      "[[7, 8], [5, 6]]\n",
      "[7, 8]\n"
     ]
    }
   ],
   "source": [
    "# 串接的列表中的元素是内容元素的引用 但是修改的时候就会修改引用\n",
    "a = [1,2]\n",
    "b = [2,3]\n",
    "c = [a,b]\n",
    "print(c[0]==a)\n",
    "print(c[1] is b)\n",
    "# 修改引用\n",
    "c[1] = [5,6]\n",
    "print(c[1] is b)\n",
    "print(c)\n",
    "print(b)\n",
    "# 修改元素本身\n",
    "# 如果是numpy可以支持[...] 更可以拓展到scaler\n",
    "c[0][:] = [7,8]\n",
    "print(c[0] is a)\n",
    "print(c)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "- 通过gradient发现我们的计算是正确的 没有出现lstm中的问题\n",
    "- 沿时间反向传播的推导思路和过程虽然有点复杂，但是代码实现难度不大\n",
    "- lstm中出现的问题应该不是在delta的反向传播上，但是具体问题还没有找到 \n",
    "- 使用identity activator + sum loss function 方便了gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
